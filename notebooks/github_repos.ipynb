{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0b9f59-0c76-4725-94f2-3a69722b2a0a",
   "metadata": {},
   "source": [
    "## Github Most Popular repos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa049b8c-c9ee-41e3-9b13-4e21f81f6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars\", \"/Drivers/SQL_Sever/jdbc/postgresql-42.7.3.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39768e5d-ff58-47a6-9b9d-088b71bcc83b",
   "metadata": {},
   "source": [
    "Data = a group of json files that has data for the top starred repos on github according to different search terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0946d932-140f-47d4-828e-a651430f974e",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ec08e95-12f2-42a3-be63-a76fede69432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, ArrayType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"created\", TimestampType()),\n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"forks\", IntegerType()),\n",
    "    StructField(\"full_name\", StringType()),\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"language\", StringType()),\n",
    "    StructField(\"open_issues\", IntegerType()),\n",
    "    StructField(\"repo_name\", StringType()),\n",
    "    StructField(\"stars\", IntegerType()),\n",
    "    StructField(\"subscribers\", IntegerType()),\n",
    "    StructField(\"topics\", ArrayType(StringType())),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"username\", StringType())\n",
    "])\n",
    "\n",
    "df = spark.read.json('work/data/*.json', schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "99c88ead-d09b-421e-a9ad-b1ac51f4545e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created: timestamp (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- forks: integer (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- open_issues: integer (nullable = true)\n",
      " |-- repo_name: string (nullable = true)\n",
      " |-- stars: integer (nullable = true)\n",
      " |-- subscribers: integer (nullable = true)\n",
      " |-- topics: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb19d2f4-6934-4929-8518-2aca53c301e4",
   "metadata": {},
   "source": [
    "## Extracting file names into a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4a6bf909-2c50-4f3a-aec3-ac29b9c97e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, regexp_replace, substring_index\n",
    "\n",
    "df_file_names = df.withColumn(\"search_term\", input_file_name())\n",
    "df_search_terms = df_file_names.withColumn(\"search_term\",regexp_replace(substring_index(\"search_term\", '/', -1), \".json\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0569c02-8e3f-4a52-9164-6faf8ba7c477",
   "metadata": {},
   "source": [
    "## Checking the number of nulls in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "74472939-51ec-455a-8a5e-ee47306676e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----+---------+---+--------+-----------+---------+-----+-----------+------+----+--------+\n",
      "|created|description|forks|full_name| id|language|open_issues|repo_name|stars|subscribers|topics|type|username|\n",
      "+-------+-----------+-----+---------+---+--------+-----------+---------+-----+-----------+------+----+--------+\n",
      "|      7|        666|   11|        1|  0|    1466|         13|        0|   11|         12|    13|   9|       9|\n",
      "+-------+-----------+-----+---------+---+--------+-----------+---------+-----+-----------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnull\n",
    "\n",
    "df_search_terms.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868da5e-c083-4a25-8bca-93ef76aa36de",
   "metadata": {},
   "source": [
    "#### Investigating the empty repo name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3415216d-2c0a-4b0a-af17-16c7df678ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----+---------+--------+--------+-----------+----------------+-----+-----------+------+----+--------+-----------+\n",
      "|created|description|forks|full_name|      id|language|open_issues|       repo_name|stars|subscribers|topics|type|username|search_term|\n",
      "+-------+-----------+-----+---------+--------+--------+-----------+----------------+-----+-----------+------+----+--------+-----------+\n",
      "|   NULL|       NULL| NULL|     NULL|63600221|    NULL|       NULL|docker-spark-arm| NULL|       NULL|  NULL|NULL|    NULL|      Spark|\n",
      "+-------+-----------+-----+---------+--------+--------+-----------+----------------+-----+-----------+------+----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_search_terms.filter(\"full_name is null\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f02d0a-a92e-41ff-9c5b-faa1137d3f7f",
   "metadata": {},
   "source": [
    "#### Filling the missing repo name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7cf7d125-acd7-445a-8227-91b620934c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled_full_name = df_search_terms.na.fill(\"afritzler/docker-spark-arm\", subset = [\"full_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f67f7-0d28-4e4d-9aef-3023d456b30d",
   "metadata": {},
   "source": [
    "## Using Github API to get missing repo details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c12a6159-7ad0-4fa9-8e4a-672873a687a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def fill_missing_values(df, col_name, github_col_name):\n",
    "    full_names_df = df.filter(isnull(col(col_name))).select(\"full_name\")\n",
    "    repo_data = {}\n",
    "    token = \"\"\n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    \n",
    "    for row in full_names_df.collect():\n",
    "        full_name = row[\"full_name\"]\n",
    "        repo_url = f\"https://api.github.com/repos/{full_name}\"\n",
    "        response = requests.get(repo_url, headers = headers)\n",
    "        if response.status_code == 200:\n",
    "            if col_name == \"type\" or col_name == \"username\":\n",
    "                repo_data[full_name] = response.json()['owner'][github_col_name]\n",
    "            else:\n",
    "                repo_data[full_name] = response.json()[github_col_name]\n",
    "        else:\n",
    "            repo_data[full_name] = None \n",
    "        \n",
    "    repo_df = spark.createDataFrame([Row(full_name=k, col_filled=v) for k, v in repo_data.items()])\n",
    "\n",
    "    res = df.join(repo_df, on=\"full_name\", how=\"left\").withColumn(\n",
    "        col_name,\n",
    "        when(col(col_name).isNull(), col(\"col_filled\"))\n",
    "        .otherwise(col(col_name))\n",
    "    ).drop(\"col_filled\")\n",
    "    \n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "49eb4c06-1694-4eaa-94d7-b47c82d023ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled_created = fill_missing_values(df_filled_full_name, \"created\",\"created_at\")\n",
    "df_filled_stars = fill_missing_values(df_filled_created, \"stars\",\"stargazers_count\")\n",
    "df_filled_issues = fill_missing_values(df_filled_stars, \"open_issues\",\"open_issues_count\")\n",
    "df_filled_subscribers = fill_missing_values(df_filled_issues, \"subscribers\",\"subscribers_count\")\n",
    "df_filled_type = fill_missing_values(df_filled_subscribers,\"type\",\"type\")\n",
    "df_filled_forks = fill_missing_values(df_filled_type, \"forks\",\"forks_count\")\n",
    "df_filled = fill_missing_values(df_filled_forks, \"username\",\"login\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ff546571-e24f-45c8-a205-7d22ce8d8e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----+---------+---+--------+-----------+---------+-----+-----------+------+----+--------+\n",
      "|created|description|forks|full_name| id|language|open_issues|repo_name|stars|subscribers|topics|type|username|\n",
      "+-------+-----------+-----+---------+---+--------+-----------+---------+-----+-----------+------+----+--------+\n",
      "|      0|        666|    0|        0|  0|    1466|          0|        0|    0|          0|    13|   0|       0|\n",
      "+-------+-----------+-----+---------+---+--------+-----------+---------+-----+-----------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filled.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2a0279-f6e2-4183-a4d4-318c7ca0442a",
   "metadata": {},
   "source": [
    "## Answering requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c69065-b125-41d2-97e7-ce0d4894be68",
   "metadata": {},
   "source": [
    "**1. Create a table for programming languages called \"programming_lang\" which has two columns, the programming language name and the number of repos using it.**\n",
    "\n",
    "Some repos have missing programming languages, this is due to some repos not containing code for example repos of popular books. Or other repos having many programming languages with no dominant language, in this case github doesn't detect a programming language for the repo. To make this more understandable we replace nulls with \"No Language Detected\".\n",
    "\n",
    "Also, Matlap language is stored once in lower case and another time on uppercase, so language names column should be standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "94676486-cc9c-49fb-98f0-7c354ac3dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, col\n",
    "\n",
    "df_filled_languages = df_filled.na.fill(\"No Language Detected\", [\"language\"])\n",
    "\n",
    "df_lower = df_filled_languages.withColumn('language', lower(col('language')))\n",
    "\n",
    "df_langs = df_lower.groupby('language').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72b6ac-5639-4207-be24-07cb2e85974f",
   "metadata": {},
   "source": [
    "Loading Data to Postgres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f1e09a7c-8d22-4dba-a922-4e10a4347ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_langs.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres:5432/github_repo\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"programmin_lang\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6285cae0-5b51-445f-887b-f8a122b0e99c",
   "metadata": {},
   "source": [
    "**2. Create a table for the organization-type accounts called \"organizations_stars\" which has two columns, the organization name and the total number of stars across all of its repos in all the files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "809139c6-ebda-4a2a-bdaf-078a18c060f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orgs = df_filled.filter(\"type = 'Organization'\").groupby('username').sum('stars').withColumnRenamed(\"sum(stars)\", \"total_stars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9d85d68f-2507-4fa3-a2c3-6023e4597e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orgs.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres:5432/github_repo\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"organizations_stars\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d2a1f-c4cd-4767-ad68-b5ff7fedf68c",
   "metadata": {},
   "source": [
    "**3. Create a table for the search terms called \"search_terms_relevance\" which has two columns, the search term - a.k.a. the file name - and the relevance score for all the repos for this search term. We use a self-defined formular for calculating the relevance where relevance score = 1.5 * forks + 1.32 * subscribers + 1.04 * stars**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cde60329-a04e-4c91-ad94-f6f6d2f51ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relevance_scores = df_filled.selectExpr(\"search_term\", \"repo_name\", \"1.5 * forks + 1.32 * subscribers + 1.04 * stars as relevance_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dd19a74a-9c12-4288-95d3-6a4c5442d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relevance_scores.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres:5432/github_repo\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"search_terms_relevance\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
